{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "Encontrarás en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [código de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el código HTML en cada solicitud para entender el tipo de información que estás obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/información solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a través de Chrome DevTools. Necesitarás identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos Útiles\n",
    "- Documentación de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de códigos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos básicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos básicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu código no se pierda en la salida.**\n",
    "\n",
    "#### Desafío 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la página de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza técnicas de manipulación de cadenas para reemplazar espacios en blanco y saltos de línea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida debería lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sebastian Raschka',\n",
       " 'lllyasviel',\n",
       " 'Belleve',\n",
       " 'Jess Frazelle',\n",
       " 'Steven Atkinson',\n",
       " 'Sanjay Viswanathan',\n",
       " 'lauren',\n",
       " 'Zihao Ye',\n",
       " 'EYHN',\n",
       " 'Mika Vilpas',\n",
       " 'Eric Niebler',\n",
       " 'Eric',\n",
       " 'Zac Sweers',\n",
       " 'dennis zhuang',\n",
       " 'Norbert de Langen',\n",
       " 'Alex Rudenko',\n",
       " 'Abhijeet Prasad',\n",
       " 'Anders Ha',\n",
       " 'Guy Bedford',\n",
       " 'Violet Hansen',\n",
       " 'Alexander Veysov',\n",
       " 'Leigh McCulloch',\n",
       " 'lijianan',\n",
       " 'Micha Reiser',\n",
       " 'J. Nick Koston']"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "import re\n",
    "\n",
    "devs = soup.find_all('h1', attrs={'class':'h3 lh-condensed'})\n",
    "name_list = []\n",
    "for desarrollador in devs:\n",
    "    nombre_con_salto = desarrollador.text.strip()\n",
    "    nombre_sin_saltos = re.sub(r'\\n.*', '', nombre_con_salto)\n",
    "    name_list.append(nombre_sin_saltos)\n",
    "name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opendatalab @MinerU',\n",
       " 'paul-gauthier @aider',\n",
       " 'freqtrade @freqtrade',\n",
       " 'lipku @metahuman-stream',\n",
       " 'dyang886 @Game-Cheats-Manager',\n",
       " 'alexta69 @metube',\n",
       " 'patched-codes @patchwork',\n",
       " 'TDAmeritrade @stumpy',\n",
       " 'healthchecks @healthchecks',\n",
       " 'dbt-labs @dbt-core',\n",
       " 'deepset-ai @haystack',\n",
       " 'cpacker @MemGPT',\n",
       " 'ansible @ansible',\n",
       " 'VikParuchuri @marker',\n",
       " 'comfyanonymous @ComfyUI',\n",
       " 'Acly @krita-ai-diffusion']"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "import re\n",
    "\n",
    "repos = soup2.find_all('h2', attrs={'class':'h3 lh-condensed'})\n",
    "repo_list = []\n",
    "for repo in repos:\n",
    "    nombre_repo = repo.text.strip()\n",
    "    nombre_limpio = nombre_repo.replace('/\\n\\n      ',\"@\")\n",
    "    repo_list.append(nombre_limpio)\n",
    "repo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 3 - Mostrar todos los enlaces de imágenes de la página de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/File:Walt_Disney_1946.JPG',\n",
       " '/wiki/File:Walt_Disney_1942_signature.svg',\n",
       " '/wiki/File:Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " '/wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " '/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " '/wiki/File:Disney_drawing_goofy.jpg',\n",
       " '/wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '/wiki/File:Walt_disney_portrait_right.jpg',\n",
       " '/wiki/File:Walt_Disney_Grave.JPG',\n",
       " '/wiki/File:DisneySchiphol1951.jpg',\n",
       " '/wiki/File:Disney1968.jpg',\n",
       " '/wiki/File:Disney_Oscar_1953_(cropped).jpg',\n",
       " '/wiki/File:Disneyland_Resort_logo.svg',\n",
       " '/wiki/File:Animation_disc.svg',\n",
       " '/wiki/File:Mickey_Mouse_colored_(head).svg',\n",
       " '/wiki/File:Blank_television_set.svg']"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "images = soup3.find_all('a', attrs={'class':'mw-file-description'})\n",
    "links_images = []\n",
    "for image in images:\n",
    "    links = image.get(\"href\")\n",
    "    links_images.append(links)\n",
    "links_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 4 - Recuperar todos los enlaces a páginas en Wikipedia que se refieren a algún tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.orghttps://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/wiki/Pythonidae',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(codename)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.orghttps://en.wikipedia.org/w/index.php?title=Python&oldid=1233294168']"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "python_refs = soup4.find_all('a', href=True)\n",
    "list_python = []\n",
    "word = 'Python'\n",
    "prefijo = 'https://en.wikipedia.org'\n",
    "for python in python_refs:\n",
    "    if word in python.text:\n",
    "        links = python.get(\"href\")\n",
    "        list_python.append(f\"{prefijo}{links}\")\n",
    "list_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 5 - Número de Títulos que han cambiado en el Código de los Estados Unidos desde su último punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "\n",
    "cambio = requests.get(f\"{url}\").text\n",
    "soup5 = BeautifulSoup(cambio, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de títulos cambiados en el Código de los EE.UU.: 3\n",
      "\n",
      "Títulos cambiados:\n",
      "\n",
      "1.- Title 5 - Government Organization and Employees ٭\n",
      "\n",
      "2.- Title 18 - Crimes and Criminal Procedure ٭\n",
      "\n",
      "3.- Title 20 - Education\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "changes = soup5.find_all('div',attrs={'class':'usctitlechanged'})\n",
    "change_list = []\n",
    "for change in changes:\n",
    "    links = change.text.strip()\n",
    "    change_list.append(links)\n",
    "\n",
    "print(f\"Total de títulos cambiados en el Código de los EE.UU.: {len(change_list)}\\n\\nTítulos cambiados:\")\n",
    "counter = 1\n",
    "for i in change_list:\n",
    "    print(f\"\\n{counter}.- {i}\")\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 6 - Una lista de Python con los diez nombres más buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "# url7 = 'https://www.fbi.gov/wanted/topten'\n",
    "url7 = 'https://en.wikipedia.org/wiki/FBI_Ten_Most_Wanted_Fugitives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup = BeautifulSoup(wanted, 'html.parser')\n",
    "wantedtag = soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nobre</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexis Flores</td>\n",
       "      <td>June 2, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bhadreshkumar Chetanbhai Patel</td>\n",
       "      <td>April 18, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alejandro Castillo</td>\n",
       "      <td>October 24, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arnoldo Jimenez</td>\n",
       "      <td>May 8, 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yulan Adonay Archaga Carias</td>\n",
       "      <td>November 3, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ruja Ignatova</td>\n",
       "      <td>June 30, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Omar Alexander Cardenas</td>\n",
       "      <td>July 20, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wilver Villegas-Palomino</td>\n",
       "      <td>April 14, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Donald Eugene Fields II</td>\n",
       "      <td>May 25, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vitel'Homme Innocent</td>\n",
       "      <td>November 15, 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Nobre              Fecha\n",
       "0                   Alexis Flores       June 2, 2007\n",
       "1  Bhadreshkumar Chetanbhai Patel     April 18, 2017\n",
       "2              Alejandro Castillo   October 24, 2017\n",
       "3                 Arnoldo Jimenez        May 8, 2019\n",
       "4     Yulan Adonay Archaga Carias   November 3, 2021\n",
       "5                   Ruja Ignatova      June 30, 2022\n",
       "6         Omar Alexander Cardenas      July 20, 2022\n",
       "7        Wilver Villegas-Palomino     April 14, 2023\n",
       "8         Donald Eugene Fields II       May 25, 2023\n",
       "9            Vitel'Homme Innocent  November 15, 2023"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "table = soup.find(\"tbody\")\n",
    "rows = table.find_all(\"div\", class_=\"center\")\n",
    "wanted_top = []\n",
    "\n",
    "for i in range(0, len(rows), 3):\n",
    "    name = rows[i].get_text()\n",
    "    date = rows[i + 1].get_text()\n",
    "    wanted_top.append([name, date])\n",
    "\n",
    "df = pd.DataFrame(wanted_top, columns=[\"Nobre\", \"Fecha\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 7 - Listar todos los nombres de idiomas y el número de artículos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\")\n",
    "languages.encoding = 'uft-8'\n",
    "soup8 = BeautifulSoup(languages.text)\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 6,847,000+ articles\n",
      "日本語: 1,421,000+ 記事\n",
      "Deutsch: 2.924.000+ Artikel\n",
      "Русский: 1 987 000+ статей\n",
      "Español: 1.965.000+ artículos\n",
      "Français: 2 621 000+ articles\n",
      "中文: 1,429,000+ 条目 / 條目\n",
      "Italiano: 1.871.000+ voci\n",
      "فارسی: ۱٬۰۰۶٬۰۰۰+ مقاله\n",
      "Português: 1.128.000+ artigos\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "language_list = []\n",
    "\n",
    "for language in langlist:\n",
    "    names = language.find(\"strong\").text.strip()\n",
    "    articles = language.find(\"small\").text.strip()\n",
    "    language_list.append((names,articles))\n",
    "for i,j in language_list:\n",
    "    print(f'{i}: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "data = soup8.find_all('h3', attrs={'class':\"govuk-heading-s dgu-topics__heading\" })\n",
    "\n",
    "data_type_list = []\n",
    "for iterator in data:\n",
    "  data_type_list.append(iterator.text.strip())\n",
    "  \n",
    "data_type_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 9 - Los 10 idiomas con más hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mandarin Chinese',\n",
       " 'Spanish',\n",
       " 'English',\n",
       " 'Arabic',\n",
       " 'Hindi',\n",
       " 'Bengali',\n",
       " 'Portuguese',\n",
       " 'Russian',\n",
       " 'Japanese',\n",
       " 'Western Punjabi',\n",
       " 'Javanese']"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "tabla = soup9.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "\n",
    "link_lang = tabla.find_all(\"a\", title=True)\n",
    "country_names = []\n",
    "for enlace in link_lang:\n",
    "    country_names.append(enlace.text.strip())\n",
    "    \n",
    "country_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "#### Desafío 10 - La información de los 20 últimos terremotos (fecha, hora, latitud, longitud y nombre de la región) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "# url7 = \"https://www.emsc-csem.org/#2\"\n",
    "url7 = 'http://ds.iris.edu/seismon/eventlist/index.phtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "response = requests.get(url7)\n",
    "soup = BeautifulSoup(response.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Latitud</th>\n",
       "      <th>Longitud</th>\n",
       "      <th>Magnitud</th>\n",
       "      <th>Profundidad</th>\n",
       "      <th>Ubicacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-JUL-2024 13:07:12</td>\n",
       "      <td>-23.54</td>\n",
       "      <td>-66.84</td>\n",
       "      <td>4.3</td>\n",
       "      <td>205</td>\n",
       "      <td>JUJUY PROVINCE, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-JUL-2024 12:25:06</td>\n",
       "      <td>-24.44</td>\n",
       "      <td>-177.12</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10</td>\n",
       "      <td>SOUTH OF FIJI ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30-JUL-2024 10:36:20</td>\n",
       "      <td>6.27</td>\n",
       "      <td>126.09</td>\n",
       "      <td>4.6</td>\n",
       "      <td>63</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30-JUL-2024 08:30:18</td>\n",
       "      <td>-56.16</td>\n",
       "      <td>-27.93</td>\n",
       "      <td>4.9</td>\n",
       "      <td>136</td>\n",
       "      <td>SOUTH SANDWICH ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30-JUL-2024 08:24:05</td>\n",
       "      <td>38.33</td>\n",
       "      <td>93.59</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11</td>\n",
       "      <td>QINGHAI, CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30-JUL-2024 07:37:53</td>\n",
       "      <td>-20.06</td>\n",
       "      <td>-177.59</td>\n",
       "      <td>4.5</td>\n",
       "      <td>492</td>\n",
       "      <td>FIJI ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30-JUL-2024 05:04:16</td>\n",
       "      <td>-21.30</td>\n",
       "      <td>-68.97</td>\n",
       "      <td>4.2</td>\n",
       "      <td>119</td>\n",
       "      <td>CHILE-BOLIVIA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30-JUL-2024 01:27:35</td>\n",
       "      <td>29.51</td>\n",
       "      <td>98.75</td>\n",
       "      <td>4.6</td>\n",
       "      <td>14</td>\n",
       "      <td>XIZANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30-JUL-2024 01:09:37</td>\n",
       "      <td>-22.37</td>\n",
       "      <td>-179.54</td>\n",
       "      <td>4.5</td>\n",
       "      <td>572</td>\n",
       "      <td>SOUTH OF FIJI ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30-JUL-2024 01:04:53</td>\n",
       "      <td>39.23</td>\n",
       "      <td>144.22</td>\n",
       "      <td>4.3</td>\n",
       "      <td>10</td>\n",
       "      <td>OFF EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30-JUL-2024 01:01:20</td>\n",
       "      <td>36.37</td>\n",
       "      <td>71.16</td>\n",
       "      <td>4.2</td>\n",
       "      <td>102</td>\n",
       "      <td>AFGHANISTAN-TAJIKISTAN BORD REG.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29-JUL-2024 22:35:11</td>\n",
       "      <td>43.70</td>\n",
       "      <td>147.81</td>\n",
       "      <td>4.8</td>\n",
       "      <td>56</td>\n",
       "      <td>KURIL ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29-JUL-2024 20:49:00</td>\n",
       "      <td>-22.38</td>\n",
       "      <td>170.86</td>\n",
       "      <td>5.3</td>\n",
       "      <td>21</td>\n",
       "      <td>SOUTHEAST OF LOYALTY ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29-JUL-2024 20:27:49</td>\n",
       "      <td>26.28</td>\n",
       "      <td>59.70</td>\n",
       "      <td>4.7</td>\n",
       "      <td>20</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29-JUL-2024 20:00:52</td>\n",
       "      <td>34.95</td>\n",
       "      <td>-116.79</td>\n",
       "      <td>4.87</td>\n",
       "      <td>7</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29-JUL-2024 17:25:51</td>\n",
       "      <td>-18.89</td>\n",
       "      <td>167.42</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10</td>\n",
       "      <td>VANUATU ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29-JUL-2024 16:25:29</td>\n",
       "      <td>32.09</td>\n",
       "      <td>131.40</td>\n",
       "      <td>5.2</td>\n",
       "      <td>49</td>\n",
       "      <td>KYUSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29-JUL-2024 14:12:03</td>\n",
       "      <td>21.70</td>\n",
       "      <td>121.03</td>\n",
       "      <td>4.7</td>\n",
       "      <td>10</td>\n",
       "      <td>TAIWAN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29-JUL-2024 13:24:23</td>\n",
       "      <td>7.99</td>\n",
       "      <td>-83.09</td>\n",
       "      <td>4.6</td>\n",
       "      <td>10</td>\n",
       "      <td>OFF COAST OF COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29-JUL-2024 13:07:12</td>\n",
       "      <td>-20.37</td>\n",
       "      <td>-173.96</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>TONGA ISLANDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Fecha Latitud Longitud Magnitud Profundidad  \\\n",
       "0   30-JUL-2024 13:07:12  -23.54   -66.84      4.3         205   \n",
       "1   30-JUL-2024 12:25:06  -24.44  -177.12      4.9          10   \n",
       "2   30-JUL-2024 10:36:20    6.27   126.09      4.6          63   \n",
       "3   30-JUL-2024 08:30:18  -56.16   -27.93      4.9         136   \n",
       "4   30-JUL-2024 08:24:05   38.33    93.59      4.7          11   \n",
       "5   30-JUL-2024 07:37:53  -20.06  -177.59      4.5         492   \n",
       "6   30-JUL-2024 05:04:16  -21.30   -68.97      4.2         119   \n",
       "7   30-JUL-2024 01:27:35   29.51    98.75      4.6          14   \n",
       "8   30-JUL-2024 01:09:37  -22.37  -179.54      4.5         572   \n",
       "9   30-JUL-2024 01:04:53   39.23   144.22      4.3          10   \n",
       "10  30-JUL-2024 01:01:20   36.37    71.16      4.2         102   \n",
       "11  29-JUL-2024 22:35:11   43.70   147.81      4.8          56   \n",
       "12  29-JUL-2024 20:49:00  -22.38   170.86      5.3          21   \n",
       "13  29-JUL-2024 20:27:49   26.28    59.70      4.7          20   \n",
       "14  29-JUL-2024 20:00:52   34.95  -116.79     4.87           7   \n",
       "15  29-JUL-2024 17:25:51  -18.89   167.42      4.9          10   \n",
       "16  29-JUL-2024 16:25:29   32.09   131.40      5.2          49   \n",
       "17  29-JUL-2024 14:12:03   21.70   121.03      4.7          10   \n",
       "18  29-JUL-2024 13:24:23    7.99   -83.09      4.6          10   \n",
       "19  29-JUL-2024 13:07:12  -20.37  -173.96      6.0          10   \n",
       "\n",
       "                           Ubicacion  \n",
       "0          JUJUY PROVINCE, ARGENTINA  \n",
       "1              SOUTH OF FIJI ISLANDS  \n",
       "2              MINDANAO, PHILIPPINES  \n",
       "3      SOUTH SANDWICH ISLANDS REGION  \n",
       "4                     QINGHAI, CHINA  \n",
       "5                FIJI ISLANDS REGION  \n",
       "6        CHILE-BOLIVIA BORDER REGION  \n",
       "7                             XIZANG  \n",
       "8              SOUTH OF FIJI ISLANDS  \n",
       "9    OFF EAST COAST OF HONSHU, JAPAN  \n",
       "10  AFGHANISTAN-TAJIKISTAN BORD REG.  \n",
       "11                     KURIL ISLANDS  \n",
       "12      SOUTHEAST OF LOYALTY ISLANDS  \n",
       "13                     SOUTHERN IRAN  \n",
       "14               SOUTHERN CALIFORNIA  \n",
       "15                   VANUATU ISLANDS  \n",
       "16                     KYUSHU, JAPAN  \n",
       "17                     TAIWAN REGION  \n",
       "18           OFF COAST OF COSTA RICA  \n",
       "19                     TONGA ISLANDS  "
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar la tabla de datos\n",
    "earthquake_list = soup.find(\"table\", class_=\"tablesorter\")\n",
    "\n",
    "# Encontrar todas las filas de la tabla\n",
    "rows = earthquake_list.find_all(\"tr\")\n",
    "\n",
    "# Lista para almacenar la información de los terremotos\n",
    "earthquake_data = []\n",
    "\n",
    "# Iterar sobre las primeras 20 filas, excluyendo la primera (que suele ser el encabezado)\n",
    "for row in rows[1:21]:\n",
    "    columnas = row.find_all(\"td\")\n",
    "    fecha = columnas[0].text.strip()\n",
    "    latitud = columnas[1].text.strip()\n",
    "    longitud = columnas[2].text.strip()\n",
    "    magnitud = columnas[3].text.strip()\n",
    "    profundidad = columnas[4].text.strip()\n",
    "    ubicacion = columnas[5].text.strip()\n",
    "    \n",
    "    earthquake_data.append([fecha, latitud, longitud, magnitud, profundidad, ubicacion])\n",
    "\n",
    "df = pd.DataFrame(earthquake_data, columns=[\"Fecha\", \"Latitud\", \"Longitud\", \"Magnitud\",\"Profundidad\",\"Ubicacion\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 11 - Datos del Top 250 de IMDB (nombre de la película, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# url11 = 'https://www.imdb.com/chart/top'\n",
    "url11 = 'https://www.filmaffinity.com/es/userlist.php?user_id=882839&list_id=1006'\n",
    "\n",
    "pelis = requests.get(url11)\n",
    "soup = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# Tu código aquí\n",
    "movies = soup.find_all(\"div\", class_=\"mc-info-container\")\n",
    "list_movies = []\n",
    "\n",
    "for row in movies[1:21]:\n",
    "    col=row.find_all(\"div\")\n",
    "    title =col[0].text.strip()\n",
    "    year =col[1].text.strip()\n",
    "    rating =col[2].text.split(\"\\n\")[1].strip()\n",
    "    director=col[5].text.strip()\n",
    "    list_movies.append([title,year,rating,director])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Año</th>\n",
       "      <th>Nota</th>\n",
       "      <th>Director</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>9,0</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino. Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>8,9</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La guerra de las galaxias. Episodio V: El impe...</td>\n",
       "      <td>1980</td>\n",
       "      <td>8,1</td>\n",
       "      <td>Irvin Kershner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La guerra de las galaxias. Episodio IV: Una nu...</td>\n",
       "      <td>1977</td>\n",
       "      <td>7,9</td>\n",
       "      <td>George Lucas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La guerra de las galaxias. Episodio VI: El ret...</td>\n",
       "      <td>1983</td>\n",
       "      <td>7,9</td>\n",
       "      <td>Richard Marquand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Indiana Jones y la última cruzada</td>\n",
       "      <td>1989</td>\n",
       "      <td>7,8</td>\n",
       "      <td>Steven Spielberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>En busca del arca perdida</td>\n",
       "      <td>1981</td>\n",
       "      <td>7,8</td>\n",
       "      <td>Steven Spielberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Casablanca</td>\n",
       "      <td>1942</td>\n",
       "      <td>8,4</td>\n",
       "      <td>Michael Curtiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Seven</td>\n",
       "      <td>1995</td>\n",
       "      <td>8,3</td>\n",
       "      <td>David Fincher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>1994</td>\n",
       "      <td>8,2</td>\n",
       "      <td>Robert Zemeckis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Salvar al soldado Ryan</td>\n",
       "      <td>1998</td>\n",
       "      <td>7,8</td>\n",
       "      <td>Steven Spielberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>El viaje de Chihiro</td>\n",
       "      <td>2001</td>\n",
       "      <td>8,1</td>\n",
       "      <td>Hayao Miyazaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>El castillo ambulante</td>\n",
       "      <td>2004</td>\n",
       "      <td>7,8</td>\n",
       "      <td>Hayao Miyazaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>La princesa Mononoke</td>\n",
       "      <td>1997</td>\n",
       "      <td>8,0</td>\n",
       "      <td>Hayao Miyazaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Érase una vez en América</td>\n",
       "      <td>1984</td>\n",
       "      <td>8,3</td>\n",
       "      <td>Sergio Leone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gran Torino</td>\n",
       "      <td>2008</td>\n",
       "      <td>8,2</td>\n",
       "      <td>Clint Eastwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "      <td>7,7</td>\n",
       "      <td>John Lasseter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>8,6</td>\n",
       "      <td>Frank Darabont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>1994</td>\n",
       "      <td>8,6</td>\n",
       "      <td>Quentin Tarantino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ratatouille</td>\n",
       "      <td>2007</td>\n",
       "      <td>7,3</td>\n",
       "      <td>Brad Bird</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titulo   Año Nota  \\\n",
       "0                                          El padrino  1972  9,0   \n",
       "1                                El padrino. Parte II  1974  8,9   \n",
       "2   La guerra de las galaxias. Episodio V: El impe...  1980  8,1   \n",
       "3   La guerra de las galaxias. Episodio IV: Una nu...  1977  7,9   \n",
       "4   La guerra de las galaxias. Episodio VI: El ret...  1983  7,9   \n",
       "5                   Indiana Jones y la última cruzada  1989  7,8   \n",
       "6                           En busca del arca perdida  1981  7,8   \n",
       "7                                          Casablanca  1942  8,4   \n",
       "8                                               Seven  1995  8,3   \n",
       "9                                        Forrest Gump  1994  8,2   \n",
       "10                             Salvar al soldado Ryan  1998  7,8   \n",
       "11                                El viaje de Chihiro  2001  8,1   \n",
       "12                              El castillo ambulante  2004  7,8   \n",
       "13                               La princesa Mononoke  1997  8,0   \n",
       "14                           Érase una vez en América  1984  8,3   \n",
       "15                                        Gran Torino  2008  8,2   \n",
       "16                                          Toy Story  1995  7,7   \n",
       "17                                    Cadena perpetua  1994  8,6   \n",
       "18                                       Pulp Fiction  1994  8,6   \n",
       "19                                        Ratatouille  2007  7,3   \n",
       "\n",
       "                Director  \n",
       "0   Francis Ford Coppola  \n",
       "1   Francis Ford Coppola  \n",
       "2         Irvin Kershner  \n",
       "3           George Lucas  \n",
       "4       Richard Marquand  \n",
       "5       Steven Spielberg  \n",
       "6       Steven Spielberg  \n",
       "7         Michael Curtiz  \n",
       "8          David Fincher  \n",
       "9        Robert Zemeckis  \n",
       "10      Steven Spielberg  \n",
       "11        Hayao Miyazaki  \n",
       "12        Hayao Miyazaki  \n",
       "13        Hayao Miyazaki  \n",
       "14          Sergio Leone  \n",
       "15        Clint Eastwood  \n",
       "16         John Lasseter  \n",
       "17        Frank Darabont  \n",
       "18     Quentin Tarantino  \n",
       "19             Brad Bird  "
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies = pd.DataFrame(list_movies, columns=[\"Titulo\", \"Año\", \"Nota\",\"Director\"])\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 12 - Nombre de la película, año y un breve resumen de las 10 películas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No se resolverlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 13 - Encontrar el reporte meteorológico en vivo (temperatura, velocidad del viento, descripción y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather in Barcelona:\n",
      "{'coord': {'lon': 2.159, 'lat': 41.3888}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}], 'base': 'stations', 'main': {'temp': 308.24, 'feels_like': 308.4, 'temp_min': 305.79, 'temp_max': 311.25, 'pressure': 1013, 'humidity': 32, 'sea_level': 1013, 'grnd_level': 1006}, 'visibility': 10000, 'wind': {'speed': 5.66, 'deg': 210}, 'clouds': {'all': 0}, 'dt': 1722357320, 'sys': {'type': 2, 'id': 18549, 'country': 'ES', 'sunrise': 1722314675, 'sunset': 1722366648}, 'timezone': 7200, 'id': 3128760, 'name': 'Barcelona', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "def weather(api_key, city):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    city_name = city\n",
    "    complete_url = f\"{base_url}appid={api_key}&q={city_name}\"\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()\n",
    "\n",
    "# Cambiar por la API personal\n",
    "api_key = '9da0f0082cf35d67520a646b8f2ff3a7'\n",
    "city_name = 'Barcelona'\n",
    "weather_data = weather(api_key, city_name)\n",
    "\n",
    "print(f\"Weather in {city_name}:\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url14)\n",
    "soup = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Libro</th>\n",
       "      <th>Precio</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Libro  Precio     Stock\n",
       "0                                A Light in the Attic  £51.77  In stock\n",
       "1                                  Tipping the Velvet  £53.74  In stock\n",
       "2                                          Soumission  £50.10  In stock\n",
       "3                                       Sharp Objects  £47.82  In stock\n",
       "4               Sapiens: A Brief History of Humankind  £54.23  In stock\n",
       "5                                     The Requiem Red  £22.65  In stock\n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34  In stock\n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93  In stock\n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60  In stock\n",
       "9                                     The Black Maria  £52.15  In stock\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99  In stock\n",
       "11                              Shakespeare's Sonnets  £20.66  In stock\n",
       "12                                        Set Me Free  £17.46  In stock\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29  In stock\n",
       "14                          Rip it Up and Start Again  £35.02  In stock\n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25  In stock\n",
       "16                                               Olio  £23.88  In stock\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59  In stock\n",
       "18                       Libertarianism for Beginners  £51.33  In stock\n",
       "19                            It's Only the Himalayas  £45.17  In stock"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "libros = list(soup.select('h1.h3 a[tittle]'))\n",
    "\n",
    "libros = soup.select('h3:nth-child(3) > a:nth-child(1)')\n",
    "precios = soup.select('div:nth-child(4) > p:nth-child(1)')\n",
    "stocks= soup.select('div:nth-child(4) > p:nth-child(2)')\n",
    "\n",
    "books=[]\n",
    "\n",
    "for libro, precio, stocks in zip(libros, precios, stocks):\n",
    "    books.append([libro[\"title\"], precio.text, stocks.text.strip()]) \n",
    "books;\n",
    "\n",
    "df = pd.DataFrame(books, columns=[\"Libro\", \"Precio\", \"Stock\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
